# Code repository for the paper titled "Towards Best Experiment Design for Evaluating Dialogue System Output" published @ INLG 2019
#More detailed readme and data files will be added soon. Cleaning some crowdsourced worked identifiable information.

```
@inproceedings{santhanam-shaikh-2019-towards,
    title = "Towards Best Experiment Design for Evaluating Dialogue System Output",
    author = "Santhanam, Sashank  and
      Shaikh, Samira",
    booktitle = "Proceedings of the 12th International Conference on Natural Language Generation",
    month = oct # "{--}" # nov,
    year = "2019",
    address = "Tokyo, Japan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-8610",
    doi = "10.18653/v1/W19-8610",
    pages = "88--94",
    abstract = "To overcome the limitations of automated metrics (e.g. BLEU, METEOR) for evaluating dialogue systems, researchers typically use human judgments to provide convergent evidence. While it has been demonstrated that human judgments can suffer from the inconsistency of ratings, extant research has also found that the design of the evaluation task affects the consistency and quality of human judgments. We conduct a between-subjects study to understand the impact of four experiment conditions on human ratings of dialogue system output. In addition to discrete and continuous scale ratings, we also experiment with a novel application of Best-Worst scaling to dialogue evaluation. Through our systematic study with 40 crowdsourced workers in each task, we find that using continuous scales achieves more consistent ratings than Likert scale or ranking-based experiment design. Additionally, we find that factors such as time taken to complete the task and no prior experience of participating in similar studies of rating dialogue system output positively impact consistency and agreement amongst raters.",
}
```
